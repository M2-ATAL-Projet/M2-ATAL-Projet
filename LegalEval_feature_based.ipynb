{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c4ae8f9",
      "metadata": {
        "id": "3c4ae8f9"
      },
      "outputs": [],
      "source": [
        "%pip install scikit-learn nltk numpy pandas spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download en_core_web_lg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2783b712-422b-43c7-9c3f-33e83d8511e9",
      "metadata": {
        "id": "2783b712-422b-43c7-9c3f-33e83d8511e9"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import spacy\n",
        "import nltk\n",
        "\n",
        "from joblib import parallel_backend\n",
        "from nltk.stem import LancasterStemmer, PorterStemmer, SnowballStemmer, WordNetLemmatizer\n",
        "from numpy import ndarray\n",
        "from pandas import DataFrame\n",
        "from sklearn.compose import make_column_transformer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_recall_fscore_support\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import FunctionTransformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd69af30",
      "metadata": {},
      "outputs": [],
      "source": [
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "#nlp = spacy.load('en_core_web_sm')\n",
        "nlp = spacy.load('en_core_web_lg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d14558d4",
      "metadata": {
        "id": "d14558d4"
      },
      "outputs": [],
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')\n",
        "data_path = 'BUILD/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b2f7e9e-3d9b-4033-be4f-17c59001bc42",
      "metadata": {
        "id": "4b2f7e9e-3d9b-4033-be4f-17c59001bc42"
      },
      "outputs": [],
      "source": [
        "train = pd.read_csv(f'{data_path}train.csv')\n",
        "dev = pd.read_csv(f'{data_path}dev.csv')\n",
        "test = pd.read_csv(f'{data_path}test.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f258c0c1-3048-4683-b46d-263e0810b6f4",
      "metadata": {
        "id": "f258c0c1-3048-4683-b46d-263e0810b6f4"
      },
      "outputs": [],
      "source": [
        "def sentence_position(df: DataFrame):\n",
        "    doc2nbsent = df.groupby('doc_id')['sentence_index'].max().to_dict()\n",
        "    df['nb_sent'] = df['doc_id'].map(doc2nbsent.get)\n",
        "    position = df['sentence_index'] / df['nb_sent']\n",
        "    return position.values.reshape(-1, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9e966b8",
      "metadata": {
        "id": "d9e966b8"
      },
      "outputs": [],
      "source": [
        "def is_verb(tag: str):\n",
        "    return tag.startswith('VB') or tag == 'MD'\n",
        "\n",
        "def verb_tense(sentence: str):\n",
        "    verbs = [tag for _, tag in nltk.pos_tag(nltk.word_tokenize(sentence)) if is_verb(tag)]\n",
        "    return '' if len(verbs) == 0 else verbs[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ac3b20a",
      "metadata": {
        "id": "1ac3b20a"
      },
      "outputs": [],
      "source": [
        "train['verb_tense'] = train['text'].map(verb_tense)\n",
        "dev['verb_tense'] = dev['text'].map(verb_tense)\n",
        "test['verb_tense'] = test['text'].map(verb_tense)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04a4b359",
      "metadata": {
        "id": "04a4b359"
      },
      "outputs": [],
      "source": [
        "verb_tense_encoding = ['VB', 'VBC', 'VBD', 'VBF', 'VBG', 'VBN', 'VBP', 'VBZ', 'MD']\n",
        "\n",
        "def verb_tense_encode(df: DataFrame):\n",
        "    return np.asarray([[1 if verb_tense_encoding[i] == vt else 0 for i in range(len(verb_tense_encoding))] for vt in df['verb_tense']])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mqut_AwAdyDo",
      "metadata": {
        "id": "mqut_AwAdyDo"
      },
      "outputs": [],
      "source": [
        "train['doc'] = np.asarray(nlp.pipe(train['text']))\n",
        "dev['doc'] = np.asarray(nlp.pipe(dev['text']))\n",
        "test['doc'] = np.asarray(nlp.pipe(test['text']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bnS7H6f5irQZ",
      "metadata": {
        "id": "bnS7H6f5irQZ"
      },
      "outputs": [],
      "source": [
        "def entity_count(entity_type: str):\n",
        "    return lambda df: np.asarray([[sum(1 for ent in doc.ents if ent.label_ == entity_type)] for doc in df['doc']])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d026a897",
      "metadata": {
        "id": "d026a897"
      },
      "outputs": [],
      "source": [
        "def sentence_length(df: DataFrame):\n",
        "    \"\"\"Donne la longueur des phrases.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): Corpus\n",
        "    \"\"\"\n",
        "    return df['text'].map(lambda x: len(x.split())).values.reshape(-1, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a526175-92b9-4c29-a40d-d731e4bec04f",
      "metadata": {
        "id": "1a526175-92b9-4c29-a40d-d731e4bec04f"
      },
      "outputs": [],
      "source": [
        "def contains_ldots(sentence: str):\n",
        "    return 1 if '...' in sentence else 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86f13059-3037-4f21-a307-bc200f8e3ac6",
      "metadata": {
        "id": "86f13059-3037-4f21-a307-bc200f8e3ac6"
      },
      "outputs": [],
      "source": [
        "train['contains_ldots'] = train['text'].map(contains_ldots)\n",
        "dev['contains_ldots'] = dev['text'].map(contains_ldots)\n",
        "test['contains_ldots'] = test['text'].map(contains_ldots)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32d28f5d",
      "metadata": {
        "id": "32d28f5d"
      },
      "outputs": [],
      "source": [
        "#stemmer = LancasterStemmer()\n",
        "#stemmer = PorterStemmer()\n",
        "#stemmer = SnowballStemmer(language='english')\n",
        "stemmer = WordNetLemmatizer()\n",
        "\n",
        "class StemmedTfidfVectorizer(TfidfVectorizer):\n",
        "    def build_analyzer(self):\n",
        "        analyzer = super().build_analyzer()\n",
        "        return lambda doc: [stemmer.lemmatize(w) for w in analyzer(doc)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f57e99cd",
      "metadata": {
        "id": "f57e99cd"
      },
      "outputs": [],
      "source": [
        "class LemmatizedTfidfVectorizer(TfidfVectorizer):\n",
        "    def build_analyzer(self):\n",
        "        #return lambda doc: [w.text for w in doc]\n",
        "        return lambda doc: [w.lemma_ for w in doc]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc185f73",
      "metadata": {
        "id": "cc185f73"
      },
      "outputs": [],
      "source": [
        "class PosTfidfVectorizer(TfidfVectorizer):\n",
        "    def build_analyzer(self):\n",
        "        analyzer = super().build_analyzer()\n",
        "        return lambda doc: [w + '/' + tag for w, tag in nltk.pos_tag(analyzer(doc))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10cc1730-c5d3-4471-b2e9-865db82ded17",
      "metadata": {
        "id": "10cc1730-c5d3-4471-b2e9-865db82ded17"
      },
      "outputs": [],
      "source": [
        "classifier = make_pipeline(\n",
        "    make_column_transformer(\n",
        "        (\n",
        "            TfidfVectorizer(stop_words='english', ngram_range=(1,3), min_df=10),\n",
        "            'text'\n",
        "        ),\n",
        "        #(\n",
        "        #    StemmedTfidfVectorizer(\n",
        "        #        stop_words='english',\n",
        "        #        ngram_range=(1,3),\n",
        "        #        min_df=10\n",
        "        #    'text'\n",
        "        #),\n",
        "        (\n",
        "            LemmatizedTfidfVectorizer(\n",
        "                stop_words='english',\n",
        "                ngram_range=(1,3),\n",
        "                min_df=10\n",
        "            ),\n",
        "            'doc'\n",
        "        ),\n",
        "        #(\n",
        "        #    PosTfidfVectorizer(\n",
        "        #        stop_words='english',\n",
        "        #        ngram_range=(1,3),\n",
        "        #        min_df=10\n",
        "        #    ),\n",
        "        #    'text'\n",
        "        #),\n",
        "        #(\n",
        "        #    'passthrough',\n",
        "        #    ['sentence_index']\n",
        "        #),\n",
        "        (\n",
        "            'passthrough',\n",
        "            ['contains_ldots']\n",
        "        ),\n",
        "        (\n",
        "            FunctionTransformer(sentence_position),\n",
        "            ['doc_id', 'sentence_index']\n",
        "        ),\n",
        "        #(\n",
        "        #    FunctionTransformer(entity_count('LOC')),\n",
        "        #    ['doc']\n",
        "        #),\n",
        "        (\n",
        "            FunctionTransformer(entity_count('LAW')),\n",
        "            ['doc']\n",
        "        ),\n",
        "        (\n",
        "            FunctionTransformer(entity_count('DATE')),\n",
        "            ['doc']\n",
        "        ),\n",
        "        #(\n",
        "        #    FunctionTransformer(entity_count('PERSON')),\n",
        "        #    ['doc']\n",
        "        #),\n",
        "        (\n",
        "            FunctionTransformer(verb_tense_encode),\n",
        "            ['verb_tense']\n",
        "        ),\n",
        "        (\n",
        "            FunctionTransformer(sentence_length),\n",
        "            ['text']\n",
        "        ),\n",
        "    ),\n",
        "    LogisticRegression(\n",
        "        multi_class='multinomial',\n",
        "        max_iter=10000\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85527abe-b1dc-4feb-9488-4cecd0140049",
      "metadata": {
        "id": "85527abe-b1dc-4feb-9488-4cecd0140049"
      },
      "outputs": [],
      "source": [
        "with parallel_backend('threading', n_jobs=4):\n",
        "    classifier.fit(train, train['labels'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51a23dbf-e639-462c-bf45-dbba06be5c59",
      "metadata": {
        "id": "51a23dbf-e639-462c-bf45-dbba06be5c59"
      },
      "outputs": [],
      "source": [
        "train['pred'] = classifier.predict(train)\n",
        "dev['pred'] = classifier.predict(dev)\n",
        "test['pred'] = classifier.predict(test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_pK9K15smg0L",
      "metadata": {
        "id": "_pK9K15smg0L"
      },
      "outputs": [],
      "source": [
        "def eval(df: DataFrame):\n",
        "    ground_truth_labels = df['labels']\n",
        "    submission_labels = df['pred']\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "        ground_truth_labels, submission_labels, average='weighted'\n",
        "    )\n",
        "    # https://pyformat.info/#number\n",
        "    print(f'{precision:.3f} & {recall:.3f} & {f1:.3f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49e90ab6-d408-4143-9eec-972e648c8d22",
      "metadata": {
        "id": "49e90ab6-d408-4143-9eec-972e648c8d22"
      },
      "outputs": [],
      "source": [
        "eval(train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce8d31c0-4603-4580-96a5-b8edbabad1e2",
      "metadata": {
        "id": "ce8d31c0-4603-4580-96a5-b8edbabad1e2"
      },
      "outputs": [],
      "source": [
        "eval(dev)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab36de46-6c9e-4cc1-ae40-dae4ae175831",
      "metadata": {
        "id": "ab36de46-6c9e-4cc1-ae40-dae4ae175831"
      },
      "outputs": [],
      "source": [
        "index2label: ndarray = classifier.named_steps['logisticregression'].classes_\n",
        "\n",
        "confusion = confusion_matrix(dev['labels'], dev['pred'], labels=index2label)\n",
        "confusion_plot = ConfusionMatrixDisplay(confusion, display_labels=index2label)\n",
        "confusion_plot.plot(xticks_rotation=60)\n",
        "_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b021225",
      "metadata": {},
      "outputs": [],
      "source": [
        "dev[['annotation_id', 'pred']].rename(columns={'pred': 'labels'}).to_csv('run1_dev.csv', index=False)\n",
        "test[['annotation_id', 'pred']].rename(columns={'pred': 'labels'}).to_csv('run1_test.csv', index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
