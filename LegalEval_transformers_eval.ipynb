{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install accelerate datasets evaluate numpy pandas scikit-learn torch transformers[torch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import ClassLabel, Features, load_dataset, TextClassification, Value\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_recall_fscore_support\n",
    "from pandas import DataFrame\n",
    "from time import time\n",
    "from torch import backends, cuda\n",
    "from transformers import pipeline\n",
    "from transformers.pipelines.pt_utils import KeyDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')\n",
    "data_path = 'BUILD/'\n",
    "\n",
    "model_name = 'distilbert'\n",
    "\n",
    "log_every = 100\n",
    "\n",
    "use_cuda_if_available = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_cuda_if_available and cuda.is_available():\n",
    "    device = \"cuda:0\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    if not use_cuda_if_available:\n",
    "        backends.cudnn.enabled = False\n",
    "        cuda.is_available = lambda : False\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_groups = ['Criminal', 'Tax']\n",
    "labels = ['PREAMBLE', 'FAC', 'RLC', 'ISSUE', 'ARG_PETITIONER', 'ARG_RESPONDENT', 'ANALYSIS', 'STA', 'PRE_RELIED', 'PRE_NOT_RELIED', 'RATIO', 'RPC', 'NONE']\n",
    "\n",
    "dataset = load_dataset(\n",
    "    'csv',\n",
    "    data_files={\n",
    "        'train': f'{data_path}train.csv',\n",
    "        'test': f'{data_path}dev.csv',\n",
    "    },\n",
    "    features=Features({\n",
    "        'doc_id': Value('uint32'),\n",
    "        'doc_index': Value('uint16'),\n",
    "        'sentence_index': Value('uint16'),\n",
    "        'annotation_id': Value('string'),\n",
    "        'text': Value('string'),\n",
    "        'meta_group': ClassLabel(names=meta_groups),\n",
    "        'labels': ClassLabel(names=labels),\n",
    "    }),\n",
    "    task=TextClassification(),\n",
    "    split='test[:]'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = pipeline('text-classification', model=f'{model_name}_model', device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_i = 0\n",
    "def log_step(x):\n",
    "    global step_i\n",
    "    if step_i % log_every == 0: print(step_i)\n",
    "    step_i += 1\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time()\n",
    "out = [log_step(x) for x in classifier(KeyDataset(dataset, 'text'))]\n",
    "elapsed = time() - start\n",
    "\n",
    "print(f'Elapsed: {elapsed}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2id = {labels[i]: i for i in range(len(labels))}\n",
    "df_out = DataFrame({\n",
    "    'labels': [int(label) for label in dataset['labels']],\n",
    "    'pred': [label2id[o['label']] for o in out],\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(df: DataFrame):\n",
    "    ground_truth_labels = df['labels']\n",
    "    submission_labels = df['pred']\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        ground_truth_labels,\n",
    "        submission_labels,\n",
    "        average='weighted',\n",
    "        zero_division=0\n",
    "    )\n",
    "    print(f'{precision:.3f} & {recall:.3f} & {f1:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(eval(df_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion = confusion_matrix(df_out['labels'], df_out['pred'], labels=labels)\n",
    "#confusion_plot = ConfusionMatrixDisplay(confusion, display_labels=labels)\n",
    "#confusion_plot.plot(xticks_rotation=60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
