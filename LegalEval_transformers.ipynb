{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c4ae8f9",
      "metadata": {
        "id": "3c4ae8f9"
      },
      "outputs": [],
      "source": [
        "%pip install accelerate datasets evaluate numpy scikit-learn torch transformers[torch]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a8c64ed",
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import ClassLabel, Features, load_dataset, TextClassification, Value\n",
        "from os import sched_getaffinity\n",
        "from torch import backends, cuda, get_num_threads, set_num_threads\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding, EvalPrediction, TrainingArguments, Trainer\n",
        "\n",
        "import evaluate\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9aa48eab",
      "metadata": {},
      "outputs": [],
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')\n",
        "data_path = 'BUILD/'\n",
        "\n",
        "#base = {'name': 'legal-bert', 'tokenizer': 'nlpaueb/legal-bert-base-uncased', 'model': 'nlpaueb/legal-bert-base-uncased'}\n",
        "base = {'name': 'distilbert', 'tokenizer': 'distilbert-base-uncased', 'model': 'distilbert-base-uncased'}\n",
        "#base = {'name': 'roberta', 'tokenizer': 'xlm-roberta-base', 'model': 'xlm-roberta-base'}\n",
        "\n",
        "epochs = 1\n",
        "batch_size = 1\n",
        "\n",
        "use_cuda_if_available = True\n",
        "gradient_checkpointing = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85a68a82",
      "metadata": {},
      "outputs": [],
      "source": [
        "num_threads = max(1, get_num_threads(), len(sched_getaffinity(0)) - 1)\n",
        "set_num_threads(num_threads)\n",
        "\n",
        "num_threads"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "624fed92",
      "metadata": {},
      "outputs": [],
      "source": [
        "if use_cuda_if_available and cuda.is_available():\n",
        "    device = \"cuda:0\"\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "    if not use_cuda_if_available:\n",
        "        backends.cudnn.enabled = False\n",
        "        cuda.is_available = lambda : False\n",
        "\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16b4b723",
      "metadata": {},
      "outputs": [],
      "source": [
        "meta_groups = ['Criminal', 'Tax']\n",
        "labels = ['PREAMBLE', 'FAC', 'RLC', 'ISSUE', 'ARG_PETITIONER', 'ARG_RESPONDENT', 'ANALYSIS', 'STA', 'PRE_RELIED', 'PRE_NOT_RELIED', 'RATIO', 'RPC', 'NONE']\n",
        "\n",
        "dataset = load_dataset(\n",
        "    'csv',\n",
        "    data_files={\n",
        "        'train': f'{data_path}train.csv',\n",
        "        'test': f'{data_path}dev.csv',\n",
        "    },\n",
        "    features=Features({\n",
        "        'doc_id': Value('uint32'),\n",
        "        'doc_index': Value('uint16'),\n",
        "        'sentence_index': Value('uint16'),\n",
        "        'annotation_id': Value('string'),\n",
        "        'text': Value('string'),\n",
        "        'meta_group': ClassLabel(names=meta_groups),\n",
        "        'labels': ClassLabel(names=labels),\n",
        "    }),\n",
        "    task=TextClassification()\n",
        ")\n",
        "\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5fec8183",
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(base['tokenizer'])\n",
        "\n",
        "tokenized_dataset = dataset.map(\n",
        "    lambda t: tokenizer(t['text'], truncation=True),\n",
        "    batched=True\n",
        ").remove_columns('text')\n",
        "\n",
        "tokenized_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7757d4c7",
      "metadata": {},
      "outputs": [],
      "source": [
        "metric = evaluate.load('f1')\n",
        "\n",
        "def compute_metrics(eval_pred: EvalPrediction):\n",
        "    predictions, references = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    return metric.compute(\n",
        "        predictions=predictions,\n",
        "        references=references,\n",
        "        average='weighted'\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09837d7a",
      "metadata": {},
      "outputs": [],
      "source": [
        "train_data = tokenized_dataset['train']#.train_test_split(train_size=0.05, stratify_by_column='labels')['train']\n",
        "trainer = Trainer(\n",
        "    AutoModelForSequenceClassification.from_pretrained(\n",
        "        base['model'],\n",
        "        num_labels=len(labels),\n",
        "        id2label={ i: labels[i] for i in range(len(labels)) },\n",
        "        label2id={ labels[i]: i for i in range(len(labels)) }\n",
        "    ).to(device),\n",
        "    TrainingArguments(\n",
        "        output_dir=f'{base[\"name\"]}_{epochs}_model',\n",
        "        evaluation_strategy='epoch',\n",
        "        num_train_epochs=epochs,\n",
        "        save_strategy='epoch',\n",
        "        #save_steps=1,\n",
        "        label_names=[\"labels\"],\n",
        "        load_best_model_at_end=True,\n",
        "        logging_dir='./logs',\n",
        "        logging_steps=10,\n",
        "        per_device_train_batch_size=batch_size,\n",
        "        per_device_eval_batch_size=batch_size,\n",
        "        gradient_accumulation_steps=8,\n",
        "        gradient_checkpointing=gradient_checkpointing,\n",
        "        learning_rate=2e-5,\n",
        "        weight_decay=0.01,\n",
        "        optim='adafactor',\n",
        "    ),\n",
        "    DataCollatorWithPadding(tokenizer=tokenizer),\n",
        "    train_data,\n",
        "    train_data,\n",
        "    tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9b2e97f",
      "metadata": {},
      "outputs": [],
      "source": [
        "#for batch in trainer.get_eval_dataloader():\n",
        "#    break\n",
        "\n",
        "#batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "#with torch.no_grad():\n",
        "#    outputs = trainer.model(**batch)\n",
        "\n",
        "#predictions = outputs.logits.cpu().numpy()\n",
        "#labels = batch[\"labels\"].cpu().numpy()\n",
        "\n",
        "#print(compute_metrics((predictions, labels)))\n",
        "\n",
        "trainer.train()\n",
        "trainer.save_model()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
